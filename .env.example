# For Docker Compose (recommended):
OLLAMA_BASE_URL=http://ollama:11434

# For local development (uncomment if not using Docker):
# OLLAMA_BASE_URL=http://localhost:11434

LLM_EXPAND_MODEL_NAME=scb10x/llama3.2-typhoon2-1b-instruct:latest
LLM_GENERATE_MODEL_NAME=llama3.1:8b
LLM_TEMPERATURE=0.2
LLM_TIMEOUT=120.0
EMBEDDING_MODEL_NAME=BAAI/bge-m3
EMBEDDING_DEVICE=cpu
RERANKER_MODEL_NAME=BAAI/bge-reranker-v2-m3
CHUNK_SIZE=1100
CHUNK_OVERLAP=200
RETRIEVAL_K=20
RERANK_TOP_N=7
DATASET_PATH=dataset/
DATABASE_PATH=database/
OUTPUT_PATH=output/