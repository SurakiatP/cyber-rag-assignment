# For Docker Compose (recommended):
# OLLAMA_BASE_URL=http://ollama:11434

# For local development (uncomment if not using Docker):
OLLAMA_BASE_URL=http://localhost:11434

LLM_EXPAND_MODEL_NAME=scb10x/typhoon2.1-gemma3-4b:latest
LLM_GENERATE_MODEL_NAME=qwen2.5:7b-instruct-q4_0
LLM_TEMPERATURE=0.2
LLM_TIMEOUT=120.0
# EMBEDDING_DEVICE=cpu
EMBEDDING_DEVICE=cuda # If you use NVIDIA GPU
EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-small
RERANKER_MODEL_NAME=BAAI/bge-reranker-base
CHUNK_SIZE=1100
CHUNK_OVERLAP=200
RETRIEVAL_K=15
RERANK_TOP_N=5
DATASET_PATH=dataset/
DATABASE_PATH=database/
OUTPUT_PATH=ingested_data/